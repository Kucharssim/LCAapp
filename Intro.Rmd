---
title: "Introduction into Latent Class Analysis"
output: html_document
---
<br>
for code and further information, visit [Github](https://github.com/Kucharssim/LCAapp) page of the project.

<script type="text/javascript">

  $(document).ready(function() {

    $('h1, h2, h3, h4, h5').each(function() {

      $(this).attr('id', $(this).html());
    });
  });
</script>

### Contents

#### 1) <a href="#What is LCA?">What is LCA?</a><br>
#### 2) <a href="#How is LCA done?">How is LCA done?</a><br>
##### <a href="#Estimating parameters">Estimating parameters</a><br>
##### <a href="#Comparing models">Comparing models</a><br>
#### 3) <a href="#How to work with the App?">How to work with the App?</a><br>
##### <a href="#Loading the data">Loading the data</a><br>
##### <a href="#Model definition">Model definition</a><br>
##### <a href="#Checking the convergence">Checking the convergence</a><br>
##### <a href="#Comparing the models">Comparing the models</a><br>
##### <a href="#Interpreting the results">Interpreting the results</a><br>
##### <a href="#Exporting the data">Exporting the data</a><br>
#### <a href="#Further reading">Further reading</a><br>

## What is LCA?

Latent Class Analysis is a statistical technique, a special case of mixture modelling. The focus of this approach is to find underlying (latent) categorical structure in the data which explains dependencies among the observed categorical variables. This method can be used if we want to find a distinct *classes* (groups) of individual cases which have similar answer patterns across all observed variables.

To ilustrate the idea, let's imagine a simple problem. Suppose you asked random people whether they will watch a 2017 Europa League final (Ajax Amsterdam vs Manchester United) and how often they put chocolate sprinkles on their toast. You observed the following frequencies (summarised in a contingency table):

```{r, echo=FALSE}
set.seed(100)

## make it 3 X 5 table
# htmldata <- data.frame(final = c(sample(c("Yes", "No"), 400, TRUE, c(0.35, 0.65)),
#                                  sample(c("Yes", "No"), 100, TRUE, c(0.15, 0.85))
#                                  ),
#                        sprinkles = c(sample(c("Often", "Very often"), 400, TRUE, c(0.5, 0.5)),
#                                  sample(c("Yes", "No"), 100, TRUE, c(0.1, 0.9))
#                                  )
# )
# tab.htmldata <- table(htmldata)
# ch.test <- chisq.test(tab.htmldata)
# #print(xtable::xtable(table(htmldata)), type="html")
# knitr::kable(tab.htmldata)
```

If you applied a test for dependency between the two variables, you would find that the two variables relate to each other (for example, $\chi^2$ test of independence yields $\chi^2$ (1) = `r #round(ch.test$statistic,2)`, p = `r #round(ch.test$p.value, 3)`). Now, here's the question: Should we conclude a genuine relationship between the two variables? Probably every reasonable person would doubt that eating sprinkles makes you watch football, or that there is any other direct relationship between those two variables. In this case, it is likely that the relationship between the variables is spurious, caused by other factor(s) we didn't observe. For example, one explanation could be that this relationship might be explained by whether a person is dutch or not (dutch people are more likely on average watch a final game featuring dutch club, and they are also more likely to put sprinkles on their toast).

And this is where LCA comes in handy, becase using this technique, we *assume* that the relationship between the variables *is spurious* and caused by mixture of different populations with different answer patterns. By conditioning on the group membership, the variables should become unrelated. (*Disclaimer: The fact that that we assume the variables unrelated under conditioning on the class membership does not mean we cannot conduct LCA on data where we do not observe relationship between the variables in the first place. That could be caused by a special case of mixture similar to a suppressor effect in regression context*)

To apply the LCA model, we need to specify how many different classes (groups) are in the data and then we can estimate a model, in which we try to optimize two distincts sets of parameters, which describe the data:

1) The relative sizes of the latent classes

2) The conditional probabilities of certain answers given a case belongs to a one of the groups

If we applied a two-class model to the data we have, we get the following results:

```{r, include=FALSE}
source("LCA.R")
source("DataHandling.R")
#fit <- emLCA(reshapeData(htmldata), 2, tol=1e-10)
```

We can see that the model estimated that people in one group (representing about 80 % of the sample) have about 40 % probability of watching the game and about 50 % of them eat sprinkles. Only 3 % of the people in the other group (20 % in the sample) will watch the game and just about 9 % of them eat sprinkles. This finding could be in line with the hypothesis that the observed data come from dutch and non-dutch population, in which the dutch people are more likely to watch football and eat sprinkles (but be avare that this interpretation takes a leap of faith, as the groups could be formed by different characteristics than just whether they are dutch or not).

Before we go ahead to show how the LCA works in detail, let us summarize what we had and what we did.

1) We had data of two categorical items, and 

2) Assumed a latent categorical structure which could explain the relationship between the variables.

3) By applying LCA, we were able to estimate the relative size of the two classes and the answer patterns conditioned on the class membership.

To put into a context, LCA is a special case of a latent variable model. It is characterized by inferring a categorical latent structure from a categorical data, which distinguishes it from Latent Profile Analysis, Factor Analysis and Item Response Theory:

But now, let's briefly explain how the method works.

## How is LCA done?

### Estimating parameters

In the previous question we showed that with LCA, we can estimate relative sizes of the latent classes and their conditional probabilities of responses on a set of manifest variables. There are many possible solutions to solve the problem, we will focus on a basic one, called Expectation-Maximisation algorithm.

Assume for a moment that when you were asking about the football match and sprinkles, you also asked whether the respondent is dutch or not. In this case, nothing would be easier than:

1) Compute the relative size of the dutch and non-dutch group in the sample (just by counting how many people are dutch from the whole sample).

2) The conditional probabilities of the responses (by splitting the data by the nationality and compute the relative frequencies of answers to the questions for both groups separately).

But how to compute it without this information? Without it, it seems the problem is tautological, because without knowing the conditional probabilities of the answers, we cannot decide who belongs to which group, and without this information, we cannot compute 1) and 2). Or can we?

And here comes the EM Algorithm, which does not require neither of the information to give the estimates. The magic comes with two steps, each using (imprecise) estimate for one of the set of information to get a better estimate of the other. The procedure goes like this:

1) *Expectation.* We start by just a random (but also can be informed) guess of the conditional probabilities. Using those, we can compute the likelihood of the data under those probabilities to obtain a probability of each case belonging to each of the classes.

2) *Maximisation.* Using the probabilities of class membership as weights, we can update the conditional probabilities of answer patterns. 

It is ensured that these two steps will *always* get closer towards some local optima. We can then iterate over those two steps until the estimates do not change (much) anymore. Usually, instead of checking whether the parameters themselves to not change, we just compute the (log)likelihood of the data under the current parameters

### Degrees of freedom, number of parameters and identifiability

when estimating the models, we should also pay attention which models we can actually compute with the information we have. Similarly to CFA (or more generally SEM), we cannot estimate more parameters than we have variables. In the LCA context, the number of variables is equal to the number of cells in the multidimensional contingency table minus one. In the example above, we have a 3 $\times$ 5 table, which means we have 14 variables. In case of 4 binary items, we would have 4 $\times$ 2 = 8 - 1 variables, and so on.

Remember that when we estimate certain model, we need to estimate two sets of parameters:

1) Proportions of the classes

2) Conditional probabilities of the answers given the class membership

For k classes, we then have to estimate k-1 proportions (as we know they have to sum to 1). The conditional probabilities have to be computed for each class and each item, but also remember they have to sum to one. The standard formula for computing the degrees of freedom (for unconstrained LCA models) is:

$$df = [(\prod_{n=1}^{N} L_{n} - 1)] - (K-1) -[K \sum_{n=1}^{N}(L_{n} - 1)]$$

Where $L_{n}$ is the number of levels on the n-th item, N the number of items and K is number of classes. For the example above, the df would be:

$df = (3 \times 5 - 1) - 1 - 2 \times (2+4) = 1$

If we have a negative degrees of freedom, we have an unidentified model, which means that we can get the same fit with different parameters (in other words, the likelihood function would have an optimum within a multidimensional space, not a single point).


### Model evaluation

After we estimated the model, we need to evaulate how it fits the data. Here we can use two approaches, which have sligthly a different goal:

- $\chi^2$ statistic tell us how well the model fits the data overall, meaning how big differences there are between the observed and implied multidimensional contingency table. Those two statistics have approximally $\chi^2$ distribution with the degrees of freedom equal to the degrees of freedom of the model. The fit can be assessed by computing the p-value associated with the statistic and degrees of freedom, and when it does not go below some arbitrary (usually 0.05) threshold, we cannot reject the model. The problem with this approach is that this test cannot really tell us that the model is good, it can only indicate whether we can or cannot reject it. It is also well known that with small samples, the $\chi^2$ approximation is imprecise leading to a low power to reject bad models, and on the other side, it tends to have a high power to reject even relatively good models with large sample size.

- $G^2$ (also called likelihood ratio or deviance statistic) is similar to the
$\chi^2$ with comparing the implied and observed frequencies. The difference is that instead of taking the differences beetween them, we calculate their ratios and log-transorm them. This statistic can also be used to compare different (nested) models, because taking the difference between $G^2$ of both models have a $\chi^2$ distribution with degrees of freedom equal to the difference of the degrees of freedom of the both models. 

- Information criteria. For the purpose of model selection, we can also use so called information criteria, which is a family of statistical indices that compare the likelihood of the data under the model and penalize it by their complexity (which on a conceptual level prevents overfitting as more complex model are always capable of better fit, even if they fit noise. The two commonly used criteria are Akaike Information Criterion (AIC) and Bayesan Information Criterion (BIc). The formulas for those two indices are:

$$AIC = - 2log(L) - 2 k$$
$$BIC = -2log(L) - log(n)k $$
Where L is the likelihood, k the number of estimated parameters and n is the sample size. Usually we want to select the model with the lowest AIC and BIC.

- Entropy of class separation. This index does not tell anything about the model fit; however, it is useful for getting an insight into how confidence we have in the class assignment. In other words, if the conditional probabilities of certain answers between some classes are very similar, we cannot be sure for large amount of cases under which case they belong. The Entropy of class separation ranges from 0 to 1. High Entropy indicates the classess are well separated, low Entropy could actually mean some classes are so similar that it is questionable whether it makes sense to distinguish between them (but the decision must be made on other characteristics than the Entropy). If we want to use the modal posterior assignment, it is usually recommended to have a high Entropy (above 0.7 or 0.8), otherwise the model has a large amount of classification error and for further analyses, it is better to model to class membership probabilities instead. The formula for the Entropy of class separation is:

$$E = 1 + \frac{1}{N log(k)} [\sum_{i=1}^{N} \sum_{k=1}^{K} P(C = k | U_{i})  log(P(C = k | U_{i}))]$$
Where C is the latent class, N the sample size and $U_{i}$ is the vector of all the answers on the items.

## How to work with the App?

### Loading the data

When you initiate the App, you should see that there is already a preloaded example dataset in case you just want to try out how the LCA works. This dataset comes from a simulation of 400 cases as a mixture of three classes with 6 binary items:

1) n = 40; probability of answering 1 is ~ 90 % for all items

2) n = 80; probability of answering 1 is ~ 15 % for all items

3) n = 280; probability of answering 1 is ~ 75 % for items 1-3 and ~ 25 % for items 4-6.

Try out the app if you can get the same results!

If you want to use your own data, beware that the current version can handle only column separated files (`.csv`) with the following specifications:

1) The first row should contain the names of the variables

2) The values can be numeric or string; if the strings contain multiple words separated by columns or spaces, they should be wrapped into a double quotes

3) The current App cannot handle NA's. If you have NA values in variables you want to include into the model, the App would think the NA is just another level of the variable (so for the time being, use only data with complete cases).

Otherwise, there is no warranty that the app works properly.

When you load the data, you should see it under the `data` panel; under the table with the raw data, you should see summary per each column in the data.

### Model definition

After you load the data, you can estimate the model(s). This is done in 4 steps:

1) *Select the variables.* By clicking on the data table, you can highlight columns (indicating variables) you want to include into the model. If you don't highlight anything, the app will use all variables. You can check which variables will be used by scrolling down to see the variable summary; the variables you did not select should dissapear.

2) *Model selection.* By clicking on the dropdown menu `Number of classes`, you can specify which model(s) you want to estimate. The app will automatically recognize how many degrees of freedom you have with the selected data and won't allow you to fit unidentified model. If you have a lot of degrees of freedom, the maximum number of classes is set to 30. Otherwise, you may estimate as many models you want.

3) *Number of replications.* By changing the number of in the numeric field, you can fit multiple replications of the same models to be able to check for convergence to the global optimum (see the next section). The default is 10, minimum 2, with undefined maximum constraints (but be aware that selecting a high number of replications could lead to a long time for the model estimation).

4) *Selecting the tolerance.* You can also change the tolerance level for the maximum likelihood estimation. The default $1\times e^{-5}$ should do fine for most of the problems, you may want to change it under some circumstances described in the next section. The minimum is $1\times e^{-10}$, maximum $1\times e^{-3}$.

After you specified the model(s), click on the button below (`Estimate Models`) to fit the models. On the bottom right, you should see a progress bar showing you the progress in the estimation.

### Checking the convergence

After you run the estimation, you should check whether the individual model(s) converged to the global optimum. On a second tab (`Model diagnostic`), you can see how many times of the total number of replications you were able to replicate the highest maximum log-likelihood of the data under the model. In case your best fit(s) did not replicate, you may want to increase the number of replications or the tolerance level by inspecting the values of the maximum log-likelihood for individual replications:

1) If the values of log-likelihood per model differ significantly (by amount of units, decimals or hundreths), try to increase the number of replications as that could mean the log-likelihood function has a lot of local minima. 

2) If the values of log-likelihood per model differ slightly, you can try to decrease the tolerance level, as the log-likelihood function may be too shallow for the current level and stops the algorithm before it reaches the minimum. Note that decreasing the tolerance level 

Because the App fits the individual replications in parallel, increasing the number of replications may not lead to a significant increase of the computational time. However, fitting a lot of replications can still lead to somewhat long waiting time. Decreasing the tolerance level always leads to increased computational time and thus it is advised to increase the number of replications instead of to change the tolerance if you run into convergence problems. If you still cannot replicate the best fit even you changed the number of replications and tolerance, that could mean the model has a lot of optima, which could lead to questioning whether the model is not overly complex for the data at hand.

### Comparing the models

On the next tab `Model comparison`, you can compare the different models you estimated. For each individual model, you will see the $\chi^2$ and $G^2$ statistic, degrees of freedom, number of estimated parameters, AIC, BIC and Entropy of class separation. The app will automatically highlight the model with the lowest BIC (but note you should pay attention also to the absolute fit indices to check if you are not actually selecting among very bad models).

By clicking on the table, you can select the model you want to export and interpret. By clicking on the last tab `Details`, you can see some information about the selected model. Note that most of it is used in the `Output` section, so you may want to skip this tab.

### Interpreting the results

In the `Output` section, you can see three tabs showing you different aspects of the selected model:

1) `Plots` In this tab, you will see graphs of the resulting model parameters. The first graph shows you the relative sized of the classes. The second tab shows you the conditional probabilities of certain answers per item, given class membership. By clicking on the box `group by items`, you can rearange the plot so you can compare the probabilities per items. By clicking on the legend in the plot, you can suppress some levels, which may help you to understand the results.

2) `Parameter Estimates` Here you can see a table with the estimated parameters. This is the same information as it is visualised in the graphs, but here you can download it as a .csv file in case you want to work with the results further.

3) `Class membership` You will see a table which shows you the probability of being in each class for every case in the data. The last column indicates the class membership selected by the modal probability assignment. You can download the table which will export a .csv file, adding k+1 columns to your original data, where k is the number of classes (for each class one column) and the class membership assignment. Note that for working with the class assignment, the model should have high Entropy, otherwise it is advised to work with the probabilistic class membership.


*If you run the app from your RStudio session in a window or in a pane, the download button may not work. For downloading the results, run the app externally in your browser.*

# Further reading

